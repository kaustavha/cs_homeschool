{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_basic.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"i96Gnqezd8an","colab_type":"text"},"cell_type":"markdown","source":["# MNIST - teaching computers to read"]},{"metadata":{"id":"BHrjJMAOmsu6","colab_type":"text"},"cell_type":"markdown","source":["First lets enable and check GPUs are working on this notebook. Should shave a few secs-mins off training time later"]},{"metadata":{"id":"M2EwVrl3nlHp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# GPU setup\n","import tensorflow as tf\n","import timeit\n","\n","# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","\n","with tf.device('/cpu:0'):\n","  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n","  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n","  net_cpu = tf.reduce_sum(net_cpu)\n","\n","with tf.device('/gpu:0'):\n","  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n","  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n","  net_gpu = tf.reduce_sum(net_gpu)\n","\n","sess = tf.Session(config=config)\n","\n","# Test execution once to detect errors early.\n","try:\n","  sess.run(tf.global_variables_initializer())\n","except tf.errors.InvalidArgumentError:\n","  print(\n","      '\\n\\nThis error most likely means that this notebook is not '\n","      'configured to use a GPU.  Change this in Notebook Settings via the '\n","      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n","  raise\n","\n","def cpu():\n","  sess.run(net_cpu)\n","  \n","def gpu():\n","  sess.run(net_gpu)\n","  \n","# Runs the op several times.\n","# print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n","#       '(batch x height x width x channel). Sum of ten runs.')\n","# print('CPU (s):')\n","# cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n","# print(cpu_time)\n","# print('GPU (s):')\n","# gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n","# print(gpu_time)\n","# print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n","\n","# sess.close()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IOMjcTxPeBpx","colab_type":"text"},"cell_type":"markdown","source":["Teaching computers to see numbers with fancy math.\n","\n","In tensorflow we first outline and create computational graphs and then execute them.\n","\n","Think of a written number input to a computer. It's 28x28 pixels We can unroll this into a simpler vector which still retains identifying information. e.g. A 8 has more lines and dark spaces than a 0."]},{"metadata":{"id":"IvCYFOpveEoI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}],"base_uri":"https://localhost:8080/","height":241},"outputId":"8871edb3-89e2-4fc0-e7f7-1c325d7f4a3b","executionInfo":{"status":"ok","timestamp":1520307796820,"user_tz":300,"elapsed":347,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["import random\n","total = []\n","for i in range(0, 10):\n","  ex = []\n","  for i in range(0, 10):\n","    n = random.randint(0, 1)\n","    ex.append(n)\n","    total.append(n)\n","  print(ex)\n","\n","print('Unrolled to:')\n","  \n","print(total)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0, 0, 1, 1, 1, 0, 0, 1, 0, 1]\n","[1, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n","[1, 0, 1, 0, 0, 1, 1, 1, 0, 0]\n","[0, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n","[1, 1, 0, 0, 1, 0, 0, 1, 0, 1]\n","[1, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n","[0, 1, 1, 0, 1, 0, 1, 0, 1, 0]\n","[1, 1, 0, 0, 0, 0, 0, 1, 0, 1]\n","[1, 1, 0, 0, 0, 1, 1, 1, 1, 1]\n","[0, 1, 1, 1, 0, 0, 0, 0, 0, 1]\n","Unrolled to:\n","[0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]\n"],"name":"stdout"}]},{"metadata":{"id":"75A0REeCFW1m","colab_type":"text"},"cell_type":"markdown","source":["First we need to get the training data. MNIST is just a large set of handwritten numbers someone painstakingly labelled.\n","\n","Tensorflow provides some nice convenient builtins for this. \n","\n","Note: The input data comes in as one_hot vectors, i.e. they're large swaths of 0s with >0s where the black marks are for numbers. These are unrolled from their normal shape into a linear shape of 768pixels "]},{"metadata":{"id":"hrIh3qM1CU7Y","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}],"base_uri":"https://localhost:8080/","height":85},"outputId":"4bbbf57a-6d98-47f1-f24f-f344d475a289","executionInfo":{"status":"ok","timestamp":1520307799442,"user_tz":300,"elapsed":795,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","import matplotlib.pyplot as plt\n","import math\n","\n","\n","DATA_DIR = '/tmp/data'\n","mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Extracting /tmp/data/train-images-idx3-ubyte.gz\n","Extracting /tmp/data/train-labels-idx1-ubyte.gz\n","Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n","Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"}]},{"metadata":{"id":"jH-5L_wdCmDa","colab_type":"text"},"cell_type":"markdown","source":["We create a placeholder to store our inputs, and a variable that will evolve with training to better predict the outcome\n","\n","We use a linear function to then map our simplified input into evidence. \n","\n","We also add a bias so we can say that some things are more independent of the input\n","\n","This is simply doing a matrix multiplication. \n"]},{"metadata":{"id":"gqKVJb1r-9Rx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["W = tf.Variable(tf.zeros([784, 10]))\n","b = tf.Variable(tf.zeros([10]))\n","x = tf.placeholder(tf.float32, [None, 784])\n","\n","y = tf.matmul(x, W) + b"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GTdigRcGGxLQ","colab_type":"text"},"cell_type":"markdown","source":["Next lets set up a placeholder to hold data incoming from the labels that we can use to refine and train our model.\n","For this we set up another placeholder with an unknown length but a shape of 10 since we have 10 distinct digits"]},{"metadata":{"id":"vClPsDYpGvRW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["y_ = tf.placeholder(tf.float32, [None, 10])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JDSsTDJxGuzk","colab_type":"text"},"cell_type":"markdown","source":["The next step is to set up the training. \n","We use softmax with logits to map our linear model into the shape we want, a probability distribution over 10 classes. \n","\n","Softmax is a generalized variant of logistic regression and instead of mapping to a binary output it maps to different classes \n","\n","A logit is an inverse sigmoid which maps from a linear 0/1 to a bernoulli probability distribtion. \n","\n","The next step is implementing the cross entropy function, which derives the differences between our model outputs and the ground truth. A lower cross entropy in this case means we are closer to the truth"]},{"metadata":{"id":"5dfYJm5sHNzO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["cross_entropy = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JY5XkG2HI9Gg","colab_type":"text"},"cell_type":"markdown","source":["Now that we've defined the computational graph:\n","input -> unroll -> matmul -> softmax -> output\n","\n","We can set up the training mechanism. \n","\n","For this we'll use gradient descent, optimizing for a reduction in cross entropy.\n","GD is basically a glorified chain rule that works via backwards propagation instead of forward propagation due to mathmetical effeciencies. This will walk our computational graph generating derivatives to track how every node affects one output. \n","\n","We use GD with a learning rate of 0.5 and tensorflow will slowly shift the variables towards the direction we want. \n","A higher learning rate may make us overshoot our ideal weights, a lower one may leave us stuck in a local minima or take forever. "]},{"metadata":{"id":"Xqv9C_SAImhc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["gd_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JzlEONgiK2Pd","colab_type":"text"},"cell_type":"markdown","source":["Now all that's left to do is create a tensorflow session and execute our graph.\n","\n","We apply parameters for the number of steps we want to use and the batch size for training i.e. how many random data points we get from our training set each step. Larger batch sizes and number of steps can lead to more accurate models\n"]},{"metadata":{"id":"zNKPvMknwYuN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def train_and_check(NUM_STEPS, MINIBATCH_SIZE):\n","  with tf.device('/gpu:0'):\n","    sess = tf.InteractiveSession()\n","    tf.global_variables_initializer().run()\n","    for _ in range(NUM_STEPS):\n","      batch_xs, batch_ys = mnist.train.next_batch(MINIBATCH_SIZE)\n","      sess.run(gd_step, feed_dict={x: batch_xs, y_: batch_ys})\n","    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","    res = accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n","    print res\n","  return res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HUs4yuMjK11r","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}],"base_uri":"https://localhost:8080/","height":34},"outputId":"49def603-7e44-4f67-dcdb-42e40cfeee27","executionInfo":{"status":"ok","timestamp":1520307817059,"user_tz":300,"elapsed":2637,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["NUM_STEPS = 3000\n","MINIBATCH_SIZE = 100\n","\n","steps = 1000\n","batch = 116\n","\n","res = []\n","data = {}\n","\n","accuracy = train_and_check(steps, batch)\n","\n","# for i in range(100, 120):\n","#     print 'results for %d steps and %d batch size' % (NUM_STEPS, i)\n","#     accuracy = train_and_check(stepsi, batchi)\n","#     data.update({accuracy: {'steps': stepsi, 'batch': batchi}})\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.9205\n"],"name":"stdout"}]},{"metadata":{"id":"T3fhJuJzLeTw","colab_type":"text"},"cell_type":"markdown","source":["Now lets see how we did. Probably 92%"]},{"metadata":{"id":"dt4FEgvP055h","colab_type":"text"},"cell_type":"markdown","source":["I wonder how model params affect accuracy\n","TODO: plot "]},{"metadata":{"id":"YBPnP1G5Lfr4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{},{}],"base_uri":"https://localhost:8080/","height":286},"outputId":"40d21ecb-6e31-4332-c601-ced78757b3dd","executionInfo":{"status":"error","timestamp":1520306650374,"user_tz":300,"elapsed":309,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n","# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","# print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n","import matplotlib.pyplot as plt\n","print 'Accuracy vs step size'\n","\n","# print steps\n","# plt.plot([v['steps'] for v in data.values() if v['batch'] == 144], [k in data.keys()])\n","# plt.show()\n","\n","# print('Accuracy vs batch size')\n","# plt.plot([v['batch'] for v in data.values() if v['steps'] == 900], data.keys())\n","# plt.show()\n","\n","\n","# plt.plot(data.values(), data.keys())\n","# plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy vs step size\n","100\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-12-214ed106be7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m144\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"]}]},{"metadata":{"id":"nrKKxUjUd4ev","colab_type":"text"},"cell_type":"markdown","source":["# Deep MNIST"]},{"metadata":{"id":"0yQX530BMi6_","colab_type":"text"},"cell_type":"markdown","source":["But the machine learning gods are displeased with 92%. And we dont want to piss of our eventual AI overlords so lets see if we can do better\n","\n","We'll dive a bit deeper and explore a multilayer convolutional networkâ‰¥\n","\n","The human brain is a place of chaos and noise, and this keeps us sane. So we'll do the same here to prevent overfitting. \n","\n","We'll be using ReLu (rectified linear unit) neurons with a small amount of noise and a slight positive intial bias to make sure theyre all alive and happy."]},{"metadata":{"id":"7KAsar95MaMb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def weight_variable(shape):\n","  initial = tf.truncated_normal(shape, stddev=0.1)\n","  return tf.Variable(initial)\n","\n","def bias_variable(shape):\n","  initial = tf.constant(0.1, shape=shape)\n","  return tf.Variable(initial)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"udgaG-H9PcbD","colab_type":"text"},"cell_type":"markdown","source":["Next up is convolution and pooling. \n","A biologically inspired if naive way to think of convolutions is the effect of your environment on you. \n","We can draw an example from neuroscience wherein the firing of a single neuron is affected by the activation and states of the neurons around it, with many stimuli converging to either create or inhibit a response. \n","\n","Alternatively it's the smoothing or smearing effect of one function on another.\n","\n","We convolute with a stride size of 1, 1 step at a time, and pad the output to maintain the same shape"]},{"metadata":{"id":"rQyYdrlIPaIA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def conv2d(x, W):\n","  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rhWqJ-upWZkf","colab_type":"text"},"cell_type":"markdown","source":["Ah pooling, here's my naive implementation of maxpool: [maxpool.py](https://github.com/kaustavha/cs_homeschool/blob/master/learn_algos/maxpool.py)\n","\n","The gist of it is, given a large matrix we move across with a preset sliding window size, gathering the max value in each window, thereby reducing the matrix"]},{"metadata":{"id":"4okj5NnoW162","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def max_pool_2x2(x):\n","  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n","                        strides=[1, 2, 2, 1], padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vbblp7roXYf0","colab_type":"text"},"cell_type":"markdown","source":["Now we can start building our CNN layer 1.  \n","We setup our neurons weights and bias.  \n","\n","This layer will compute 32 output features for each 5x5 patch of the incoming tensor. We'll have one input channel into the neuron, and output channels equal to the number of features we compute. You can think of features are important or distinguishing characteristics in the image. \n","\n","We also create a bias variable with the shape of our output feature set. \n","\n","I think of this similiar to a biological neuron convoluting incoming visual info and sending it forward to different neurons for different purposes. Someone with terrible handwriting will probably understand bad handwriting better, and neurons wired towards sex steroid generation will care about different things than those wired towards hunger centers.  \n"]},{"metadata":{"id":"a3UcNzUVZQki","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["W_conv1 = weight_variable([5, 5, 1, 32])\n","b_conv1 = bias_variable([32])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dwjb55ncXCSq","colab_type":"text"},"cell_type":"markdown","source":["Now to apply the layer we begin by reshaping our incoming image to a 4D vector, and add information regarding width, height, color channels.\n"]},{"metadata":{"id":"zs4haTvpaYnd","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["x_image = tf.reshape(x, [-1,28,28,1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GiST5LtiXYG9","colab_type":"text"},"cell_type":"markdown","source":["Now we pass the image through our relu neurons with the pre-set weight and bias then run maxpool over the output. \n","This will reduce the size of the matrix to 14x14 since we are taking 2x2 sized windows and concentrate information our NN cares about"]},{"metadata":{"id":"9XHdKjHVa9dN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n","h_pool1 = max_pool_2x2(h_conv1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C7lUqCarbIQL","colab_type":"text"},"cell_type":"markdown","source":["Now we add a second layer. Similiar to the last one, further reducing our image size and increasing the number of generated features"]},{"metadata":{"id":"q0GPDpfGbNvt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["W_conv2 = weight_variable([5, 5, 32, 64])\n","b_conv2 = bias_variable([64])\n","\n","h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n","h_pool2 = max_pool_2x2(h_conv2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LOYfeCTbbeaL","colab_type":"text"},"cell_type":"markdown","source":["Now we add a slightly different special layer. A densely connected layer. With 1024 neurons this layer can process the whole image and benefit from the feature generation from previous layers.\n","\n","This is also in some ways biologically inspired, neuronal tracts exist that run the length of the brain connecting distal zones. Although we're not sure if they play a role in processing or merely ferrying. \n","\n","We have to once again reshape the incoming vector, apply our weights and bias and run it through the relu function"]},{"metadata":{"id":"lsa7hY6zcB4Z","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["W_fc1 = weight_variable([7 * 7 * 64, 1024])\n","b_fc1 = bias_variable([1024])\n","\n","h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n","h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JsM91qwEbM8s","colab_type":"text"},"cell_type":"markdown","source":["To reduce overfitting further from what we managed with our biases and weights, we apply a dropout layer. \n","\n","This layer will be turned off during testing but will run during training. \n","Relaxed learning but concentrated focused testing. \n","\n","The paper behind dropout mentions inspiration from biological reproductive fitness and how genes are dropped when offspring are created. \n","There's also neuroscientific inspiration wherein we see a reduction in neuronal density and connections from pruning as people age which results in improved performance over infantile over-activated states."]},{"metadata":{"id":"-paZlhqNcEUX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["keep_prob = tf.placeholder(tf.float32)\n","h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E791Kau5dtVh","colab_type":"text"},"cell_type":"markdown","source":["Finally we add a readout layer. Similiar to the softmax layer which gathered the inputs and "]},{"metadata":{"id":"fxD63jTEfUcQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["W_fc2 = weight_variable([1024, 10])\n","b_fc2 = bias_variable([10])\n","\n","y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I0h_lsHzfYgU","colab_type":"text"},"cell_type":"markdown","source":["One main difference is that we'll be replacing the simpler gradient descent optimization technique with ADAM. \n","\n","Unlike GD which maintains a constant learning rate ADAM computes individual adaptive learning rates for different paremeters from estimates of the exponential moving average of the gradient, squared gradient and parameters beta1 and 2, the first and second moments of the gradient.\n","\n","ADAM outperforms :   \n"," RMSProp - does well on noisy problems   \n"," AdaGrad - does well on NLP & CV problems   "]},{"metadata":{"id":"vGDtA_WggxDh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}],"base_uri":"https://localhost:8080/","height":544},"outputId":"f3bef7e3-9223-4125-8b59-ebdafecd9fc4","executionInfo":{"status":"ok","timestamp":1520308138027,"user_tz":300,"elapsed":294880,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["with tf.device('/gpu:0'):\n","  \n","  cross_entropy = tf.reduce_mean(\n","      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n","\n","  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n","  correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","  sess = tf.InteractiveSession()\n","  tf.global_variables_initializer().run()\n","  # sess.run(tf.global_variables_initializer())\n","  for i in range(30000):\n","    batch_xs, batch_ys = mnist.train.next_batch(64)\n","    if i%1000 == 0:\n","      train_accuracy = accuracy.eval(feed_dict={\n","          x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n","      print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n","    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n","\n","  print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n","    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["step 0, training accuracy 0.03125\n","step 1000, training accuracy 0.96875\n","step 2000, training accuracy 0.984375\n","step 3000, training accuracy 0.96875\n","step 4000, training accuracy 0.984375\n","step 5000, training accuracy 0.984375\n","step 6000, training accuracy 1\n","step 7000, training accuracy 1\n","step 8000, training accuracy 1\n","step 9000, training accuracy 1\n","step 10000, training accuracy 1\n","step 11000, training accuracy 1\n","step 12000, training accuracy 1\n","step 13000, training accuracy 1\n","step 14000, training accuracy 1\n","step 15000, training accuracy 1\n","step 16000, training accuracy 1\n","step 17000, training accuracy 1\n","step 18000, training accuracy 1\n","step 19000, training accuracy 1\n","step 20000, training accuracy 1\n","step 21000, training accuracy 1\n","step 22000, training accuracy 1\n","step 23000, training accuracy 1\n","step 24000, training accuracy 1\n","step 25000, training accuracy 1\n","step 26000, training accuracy 1\n","step 27000, training accuracy 1\n","step 28000, training accuracy 1\n","step 29000, training accuracy 1\n","test accuracy 0.9931\n"],"name":"stdout"}]},{"metadata":{"id":"DyzucOubsYuA","colab_type":"text"},"cell_type":"markdown","source":["We'll probably get around 99.2% accuracy. Up 7%"]},{"metadata":{"id":"t2Ced_PRnOE8","colab_type":"text"},"cell_type":"markdown","source":["# Slim and nielsen net"]},{"metadata":{"id":"9GpV9wfOnZnR","colab_type":"text"},"cell_type":"markdown","source":["Todo: Slim and Nielsen net explanation"]},{"metadata":{"id":"mDs3bbdxnRyU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import tensorflow.contrib.slim as slim\n","MEAN = np.mean(mnist.train.images)\n","STD = np.std(mnist.train.images)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OoLaxOnDn1Br","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Convenience method for reshaping images. The included MNIST dataset stores images\n","# as Nx784 row vectors. This method reshapes the inputs into Nx28x28x1 images that are\n","# better suited for convolution operations and rescales the inputs so they have a\n","# mean of 0 and unit variance.\n","import numpy as np\n","def resize_images(images):\n","    reshaped = (images - MEAN)/STD\n","    reshaped = np.reshape(reshaped, [-1, 28, 28, 1])\n","    \n","    assert(reshaped.shape[1] == 28)\n","    assert(reshaped.shape[2] == 28)\n","    assert(reshaped.shape[3] == 1)\n","    \n","    return reshaped\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"T47u1J-jn7js","colab_type":"text"},"cell_type":"markdown","source":["Nielsent net:\n","- 28x28 input\n","- conv layer w/ 20 kernels, stride = 1, size=5\n","- 2x2 maxpool\n","- conv lyer with 40 kernels, stride=1, size=5\n","- 2x2 maxpool\n","- fully connected layer w/ 1000 hidden units and dropout\n","- 2nd fully connected layer, same as above\n","- output layer of 10\n","- Trained with momentumOptimizer"]},{"metadata":{"id":"J9BQXEfqQGqT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def nielsen_net(inputs, is_training, scope='NielsenNet'):\n","    with tf.variable_scope(scope, 'NielsenNet'):\n","        # First Group: Convolution + Pooling 28x28x1 => 28x28x20 => 14x14x20\n","        net = slim.conv2d(inputs, 20, [5, 5], padding='SAME', scope='layer1-conv')\n","        net = slim.max_pool2d(net, 2, stride=2, scope='layer2-max-pool')\n","\n","        # Second Group: Convolution + Pooling 14x14x20 => 10x10x40 => 5x5x40\n","        net = slim.conv2d(net, 40, [5, 5], padding='VALID', scope='layer3-conv')\n","        net = slim.max_pool2d(net, 2, stride=2, scope='layer4-max-pool')\n","\n","        # Reshape: 5x5x40 => 1000x1\n","        net = tf.reshape(net, [-1, 5*5*40])\n","\n","        # Fully Connected Layer: 1000x1 => 1000x1\n","        net = slim.fully_connected(net, 1000, scope='layer5')\n","        net = slim.dropout(net, is_training=is_training, scope='layer5-dropout')\n","\n","        # Second Fully Connected: 1000x1 => 1000x1\n","        net = slim.fully_connected(net, 1000, scope='layer6')\n","        net = slim.dropout(net, is_training=is_training, scope='layer6-dropout')\n","\n","        # Output Layer: 1000x1 => 10x1\n","        net = slim.fully_connected(net, 10, scope='output')\n","        net = slim.dropout(net, is_training=is_training, scope='output-dropout')\n","\n","        return net"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GKEjkVnGQOhx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["with tf.device('/gpu:0'):\n","  tf.reset_default_graph()\n","  sess = tf.InteractiveSession()\n","  tf.global_variables_initializer().run()\n","\n","  # Create the placeholder tensors for the input images (x), the training labels (y_actual)\n","  # and whether or not dropout is active (is_training)\n","  x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='Inputs')\n","  y_actual = tf.placeholder(tf.float32, shape=[None, 10], name='Labels')\n","  is_training = tf.placeholder(tf.bool, name='IsTraining')\n","\n","  # Pass the inputs into nielsen_net, outputting the logits\n","  logits = nielsen_net(x, is_training, scope='NielsenNetTrain')\n","  # Use the logits to create four additional operations:\n","  #\n","  # 1: The cross entropy of the predictions vs. the actual labels\n","  # 2: The number of correct predictions\n","  # 3: The accuracy given the number of correct predictions\n","  # 4: The update step, using the MomentumOptimizer\n","  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_actual))\n","  correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_actual, 1))\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","  \n","  train_step = tf.train.MomentumOptimizer(0.01, 0.5).minimize(cross_entropy)\n","  \n","  # To monitor our progress using tensorboard, create two summary operations\n","  # to track the loss and the accuracy\n","  loss_summary = tf.summary.scalar('loss', cross_entropy)\n","  accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n","\n","  sess.run(tf.global_variables_initializer())\n","  train_writer = tf.summary.FileWriter('/tmp/nielsen-net', sess.graph)\n","  \n","\n","  eval_data = {\n","      x: resize_images(mnist.validation.images),\n","      y_actual: mnist.validation.labels,\n","      is_training: False\n","  }\n","\n","  for i in xrange(100000):\n","    images, labels = mnist.train.next_batch(100)\n","    summary, _ = sess.run([loss_summary, train_step], feed_dict={x: resize_images(images), y_actual: labels, is_training: True})\n","    train_writer.add_summary(summary, i)\n","    \n","    if i % 1000 == 0:\n","        summary, acc = sess.run([accuracy_summary, accuracy], feed_dict=eval_data)\n","        train_writer.add_summary(summary, i)\n","        print(\"Step: %5d, Validation Accuracy = %5.2f%%\" % (i, acc * 100))\n","\n","  test_data = {\n","      x: resize_images(mnist.test.images),\n","      y_actual: mnist.test.labels,\n","      is_training: False\n","   }\n","\n","  acc = sess.run(accuracy, feed_dict=test_data)\n","\n","  print(\"Test Accuracy = %5.2f%%\" % (100 * acc))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6LWelzcsQOOh","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"itwIRDXCQS8I","colab_type":"text"},"cell_type":"markdown","source":["# Nielsen net with more layers and ADAM, WIP"]},{"metadata":{"id":"mPJuW0GKn21Z","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def nielsen_net(inputs, is_training, scope='NielsenNet'):\n","    with tf.variable_scope(scope, 'NielsenNet'):\n","        # First Group: Convolution + Pooling 28x28x1 => 28x28x20 => 14x14x20\n","        net = slim.conv2d(inputs, 20, [5, 5], padding='SAME', scope='layer1-conv')\n","        net = slim.max_pool2d(net, 2, stride=2, scope='layer2-max-pool')\n","\n","        # Second Group: Convolution + Pooling 14x14x20 => 10x10x40 => 5x5x40\n","        net = slim.conv2d(net, 40, [5, 5], padding='VALID', scope='layer3-conv')\n","        net = slim.max_pool2d(net, 2, stride=2, scope='layer4-max-pool')\n","        \n","\n","        # Reshape: 5x5x40 => 1000x1\n","        net = tf.reshape(net, [-1, 5*5*40])\n","        \n","        # MLP\n","#         net = slim.stack(net, slim.fully_connected, [1000,1024,2048], scope='fc')\n","\n","#         # Fully Connected Layer: 1000x1 => 1000x1\n","        net = slim.fully_connected(net, 1000, scope='layer5')\n","        net = slim.dropout(net, is_training=is_training, scope='layer5-dropout')\n","\n","#         # Second Fully Connected: 1000x1 => 1000x1\n","        net = slim.fully_connected(net, 1000, scope='layer6')\n","        net = slim.dropout(net, is_training=is_training, scope='layer6-dropout')\n","\n","#         # Second Fully Connected: 1000x1 => 1000x1\n","#         net = slim.fully_connected(net, 1000, scope='layer7')\n","#         net = slim.dropout(net, is_training=is_training, scope='layer7-dropout')\n","\n","        # Output Layer: 1000x1 => 10x1\n","        net = slim.fully_connected(net, 10, scope='output')\n","        net = slim.dropout(net, is_training=is_training, scope='output-dropout')\n","\n","        return net"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EFC-w9JzXrq9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def mcdnnSingle(inputs, is_training, scope='mcdnn'):\n","  with tf.variable_scope(scope, 'mcdnn'):\n","      net = slim.conv2d(inputs, 20, [4, 4], padding='SAME', scope='layer1-conv')\n","      net = slim.max_pool2d(net, 2, stride=2, scope='layer2-max-pool')\n","      \n","      net = slim.conv2d(inputs, 40, [5, 5], padding='SAME', scope='layer3-conv')\n","      net = slim.max_pool2d(net, 3, stride=3, scope='layer4-max-pool')\n","      \n","      net = slim.fully_connected(net, 150, scope='layer5-fully-connected')\n","      \n","      net = slim.fully_connected(net, 10, scope='output')\n","      \n","      return net\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"snEwmOEwXnIN","colab_type":"text"},"cell_type":"markdown","source":["def mcdnnSingle:\n"]},{"metadata":{"id":"jL_QhoPLohGs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}],"base_uri":"https://localhost:8080/","height":1816},"outputId":"fb476651-95e4-412b-a374-05c0d64170a2","executionInfo":{"status":"error","timestamp":1520307619683,"user_tz":300,"elapsed":1017,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["with tf.device('/gpu:0'):\n","  tf.reset_default_graph()\n","  sess = tf.InteractiveSession()\n","  tf.global_variables_initializer().run()\n","\n","  # Create the placeholder tensors for the input images (x), the training labels (y_actual)\n","  # and whether or not dropout is active (is_training)\n","  x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='Inputs')\n","  y_actual = tf.placeholder(tf.float32, shape=[None, 10], name='Labels')\n","  is_training = tf.placeholder(tf.bool, name='IsTraining')\n","\n","  # Pass the inputs into nielsen_net, outputting the logits\n","#   logits = nielsen_net(x, is_training, scope='NielsenNetTrain')\n","  \n","  logits = mcdnnSingle(x, is_training, scope='mcdnn')\n","  # Use the logits to create four additional operations:\n","  #\n","  # 1: The cross entropy of the predictions vs. the actual labels\n","  # 2: The number of correct predictions\n","  # 3: The accuracy given the number of correct predictions\n","  # 4: The update step, using the MomentumOptimizer\n","  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_actual))\n","  correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_actual, 1))\n","  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","  \n","  # swap put momentum for adam\n","  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n","  \n","#   train_step = tf.train.MomentumOptimizer(0.01, 0.5).minimize(cross_entropy)\n","  \n","  # To monitor our progress using tensorboard, create two summary operations\n","  # to track the loss and the accuracy\n","  loss_summary = tf.summary.scalar('loss', cross_entropy)\n","  accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n","\n","  sess.run(tf.global_variables_initializer())\n","  train_writer = tf.summary.FileWriter('/tmp/nielsen-net', sess.graph)\n","  \n","\n","  eval_data = {\n","      x: resize_images(mnist.validation.images),\n","      y_actual: mnist.validation.labels,\n","      is_training: False\n","  }\n","  \n","  steps = 900 # original is 100k\n","  batch = 112 # original is 100\n","\n","  \n","  for i in xrange(steps):\n","    images, labels = mnist.train.next_batch(batch)\n","    summary, _ = sess.run([loss_summary, train_step], feed_dict={x: resize_images(images), y_actual: labels, is_training: True})\n","    train_writer.add_summary(summary, i)\n","    \n","    if i % 1000 == 0:\n","        summary, acc = sess.run([accuracy_summary, accuracy], feed_dict=eval_data)\n","        train_writer.add_summary(summary, i)\n","        print(\"Step: %5d, Validation Accuracy = %5.2f%%\" % (i, acc * 100))\n","\n","  test_data = {\n","      x: resize_images(mnist.test.images),\n","      y_actual: mnist.test.labels,\n","      is_training: False\n","   }\n","\n","  acc = sess.run(accuracy, feed_dict=test_data)\n","\n","  print(\"Test Accuracy = %5.2f%%\" % (100 * acc))\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-43-5d73ed949a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_actual\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[9072,10] labels_size=[112,10]\n\t [[Node: softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](softmax_cross_entropy_with_logits_sg/Reshape, softmax_cross_entropy_with_logits_sg/Reshape_1)]]\n\t [[Node: Mean/_67 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_371_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'softmax_cross_entropy_with_logits_sg', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-43-5d73ed949a6b>\", line 22, in <module>\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_actual))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 250, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1960, in softmax_cross_entropy_with_logits\n    labels=labels, logits=logits, dim=dim, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1875, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4575, in _softmax_cross_entropy_with_logits\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[9072,10] labels_size=[112,10]\n\t [[Node: softmax_cross_entropy_with_logits_sg = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](softmax_cross_entropy_with_logits_sg/Reshape, softmax_cross_entropy_with_logits_sg/Reshape_1)]]\n\t [[Node: Mean/_67 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_371_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"]}]},{"metadata":{"id":"q00WlzJyvFKr","colab_type":"text"},"cell_type":"markdown","source":["comes to 99.45%\n"]},{"metadata":{"id":"fNGDGrklnWS4","colab_type":"text"},"cell_type":"markdown","source":["TODO: try to use the trained model to read numbers stored in our local drive instance\n","\n","Input\n","25 4 2\n","Expected Output\n","THINK,OUTTHINK,THINK,THINK,THINK,OUTTHINK,19,18,17,OUT,15,14,13,OUTTHINK,11,10,9,OUT,7,6,5,OUT,3,THINK,1\n"]},{"metadata":{"id":"wAh_ao_hnFMg","colab_type":"text"},"cell_type":"markdown","source":["# Using the model to identify new input numbers"]},{"metadata":{"id":"j92XR563xYRQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{}],"base_uri":"https://localhost:8080/","height":51},"outputId":"0646d17b-4dce-4fc9-fed8-b53aa6488061","executionInfo":{"status":"ok","timestamp":1520213307417,"user_tz":300,"elapsed":2088,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# PyDrive reference:\n","# https://googledrive.github.io/PyDrive/docs/build/html/index.html\n","\n","# 2. Create & upload a file text file.\n","uploaded = drive.CreateFile({'title': 'Sample upload.txt'})\n","uploaded.SetContentString('Sample upload file content')\n","uploaded.Upload()\n","print('Uploaded file with ID {}'.format(uploaded.get('id')))\n","\n","# 3. Load a file by ID and print its contents.\n","downloaded = drive.CreateFile({'id': uploaded.get('id')})\n","print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Uploaded file with ID 1thyxF-wigRKE50_tsBlK0bUgAblKtDon\n","Downloaded content \"Sample upload file content\"\n"],"name":"stdout"}]},{"metadata":{"id":"7U40qVIRxviq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":347},"outputId":"d20dd1de-a4f4-4b47-9fe1-0d0fec134c81","executionInfo":{"status":"ok","timestamp":1520356115322,"user_tz":300,"elapsed":1037,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["from google.colab import auth\n","auth.authenticate_user()\n","from googleapiclient.discovery import build\n","drive_service = build('drive', 'v3')\n","from PIL import Image\n","\n","\n","file_id = '13M-dLDt5hmG3bxmfBol_W5UnjyGU40lK'\n","\n","import io\n","from googleapiclient.http import MediaIoBaseDownload\n","\n","request = drive_service.files().get_media(fileId=file_id)\n","downloaded = io.BytesIO()\n","downloader = MediaIoBaseDownload(downloaded, request)\n","done = False\n","while done is False:\n","  # _ is a placeholder for a progress object that we ignore.\n","  # (Our file is small, so we skip reporting progress.)\n","  _, done = downloader.next_chunk()\n","downloaded.seek(0)\n","img_str = downloaded.read()\n","dd = io.BytesIO(img_str)\n","# downloaded.seek(0)\n","# file = downloaded.read()\n","\n","# import locale\n","# locale.getdefaultlocale()\n","# file1_open = open(file, encoding=locale.getdefaultlocale()[1])\n","# file1_content = file1_open.read()\n","\n","# print('Downloaded file contents are: {}'.format(file1_content))\n","pil_im = Image.open(dd)\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import numpy as np\n","\n","size = 28,28\n","pil_im.thumbnail(size, Image.ANTIALIAS)\n","\n","imgplot = plt.imshow(pil_im)\n","\n","# print('Downloaded file contents are: {}'.format(downloaded.read()))"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAVQAAAFKCAYAAABCTqdeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAG29JREFUeJzt3X9QVXX+x/HXlSsa/r7ERR1XbVlL\nIy1t/YGMmIaa7lQ2W4kOmrO0+QvxN/5YFfdLSiBZSk0qmm2SLiPb7lDbDk677U6jCGaOpk6DPyYH\nWcML/iQ1hDnfP5zueANFDh/w4j4ff3k+n/Px877nnvvinHvPPddhWZYlAECDtbjXBQDA/YJABQBD\nCFQAMIRABQBDCFQAMIRABQBDCFQAMIRABQBDCFQAMMR2oK5Zs0YTJkxQTEyMDh8+7NO3d+9evfji\ni5owYYLefffdBhcJAM2BrUAtLCzU6dOnlZ2drdWrV2v16tU+/a+//royMjK0c+dO7dmzRydOnDBS\nLAD4M1uBmp+fr+joaElSWFiYLl26pIqKCklScXGxOnTooC5duqhFixYaPny48vPzzVUMAH7KVqCW\nlZWpU6dO3mWXyyWPxyNJ8ng8crlctfYBwP3MyIdS3LAKAGwGqtvtVllZmXf53LlzCgkJqbWvtLRU\nbre7gWUCgP+zFaiRkZHKy8uTJB09elRut1tt27aVJHXr1k0VFRU6c+aMqqqq9MUXXygyMtJcxQDg\npxx2bzCdnp6ur776Sg6HQ0lJSTp27JjatWunUaNGaf/+/UpPT5ckjR49WnFxcUaLBgB/ZDtQAQC+\n+KYUABhCoAKAIQQqABhCoAKAIQQqABhCoAKAIQQqABhCoAKAIQQqABhCoAKAIQQqABhCoAKAIQQq\nABhCoAKAIQQqABhCoAKAIQQqABhCoAKAIQQqABhCoAKAIQQqABhCoAKAIQQqABhCoAKAIQQqABji\ntDswLS1NBw4cUFVVlaZNm6bRo0d7+0aOHKnOnTsrICBAkpSenq7Q0NCGVwsAfsxWoO7bt0/Hjx9X\ndna2Lly4oBdeeMEnUCUpMzNTbdq0MVIkADQHtgJ14MCB6tevnySpffv2unbtmqqrq71HpADwv8hW\noAYEBCgoKEiSlJOTo6ioqBphmpSUpJKSEj355JNasGCBHA5Hw6sFAD9m+z1USfr888+Vk5Oj999/\n36c9ISFBw4YNU4cOHTRr1izl5eXpmWeeaVChAODvbH/K/+WXX2rjxo3KzMxUu3btfPrGjx+v4OBg\nOZ1ORUVFqaioqMGFAoC/sxWoV65cUVpamjZt2qSOHTvW6IuLi1NlZaUkaf/+/erVq1fDKwUAP2fr\nlP+zzz7ThQsXNHfuXG/b4MGD9cgjj2jUqFGKiorShAkT1KpVKz366KOc7gP4n+CwLMu610UAwP2A\nb0oBgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqAC\ngCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCFO\nO4MKCgo0Z84c9erVS5L08MMPa8WKFd7+vXv3at26dQoICFBUVJRmzZplploA8GO2AlWSBg0apA0b\nNtTa9/rrr2vr1q0KDQ1VbGysxowZo1/96le2iwSA5sD4KX9xcbE6dOigLl26qEWLFho+fLjy8/NN\nTwMAfsd2oJ44cULTp0/XxIkTtWfPHm+7x+ORy+XyLrtcLnk8noZVCQDNgK1T/p49eyo+Pl5jx45V\ncXGxpkyZot27dyswMNB0fQDQbNg6Qg0NDdW4cePkcDjUvXt3PfjggyotLZUkud1ulZWVedctLS2V\n2+02Uy0A+DFbgZqbm6utW7dKunmKX15ertDQUElSt27dVFFRoTNnzqiqqkpffPGFIiMjzVUMAH7K\nYVmWVd9BFRUVWrhwoS5fvqwbN24oPj5e5eXlateunUaNGqX9+/crPT1dkjR69GjFxcUZLxwA/I2t\nQAUA1MQ3pQDAEAIVAAyx/U0pwI4zZ87YGpeRkXHbvtTUVC1evLhG+3fffWdrLjsfoiYkJOjHH3+s\n97hWrVrVewz8F0eoAGAIgQoAhhCoAGAIgQoAhhCoAGAIgQoAhhCoAGAIgQoAhhCoAGAIgQoAhhCo\nAGAIgQoAhnBzFDTIrT93czd+8Ytf2JpnxYoVd+yv7SYjCQkJtuZ67bXX6j0mISFBixYtqve42/0U\nO5onjlABwBACFQAMIVABwBACFQAMIVABwBACFQAMIVABwBACFQAMIVABwBBb35TatWuXcnNzvctH\njhzRwYMHvcvh4eEaMGCAd/mDDz5QQEBAA8oEAP9nK1BfeuklvfTSS5KkwsJC/eMf//Dpb9u2rbZv\n397w6gCgGWnwKf+7776rmTNnmqgFAJq1BgXq4cOH1aVLF4WEhPi0V1ZWasGCBYqJidG2bdsaVCAA\nNBcOy7Isu4NXrlyp3/zmNxo8eLBP+86dO/Xcc8/J4XAoNjZWf/zjH9W3b98GFwsA/qxBgTpmzBh9\n8sknCgwMvO06aWlpCgsL029/+1u708CPbdy4sV7rHzt2zNY8TXmbu++//77eYzp37qwtW7bUe9zv\nfve7eo+RpBYtuEDHH9l+VkpLS9WmTZsaYXrq1CktWLBAlmWpqqpKX3/9tXr16tXgQgHA39m+wbTH\n45HL5fIub968WQMHDlT//v3VuXNnvfjii2rRooVGjhypfv36GSkWAPyZ7UB97LHHfE5xbr3LuZ07\nlwNAc8cbMQBgCIEKAIYQqABgCIEKAIYQqABgCIEKAIYQqABgCIEKAIY06Lv8wKuvvlqv9S9evGhr\nnj59+ty2Lzk5WStWrKjRvn79eltzJScn13vMnDlzVF1dXe9x3Hj9/sIRKgAYQqACgCEEKgAYQqAC\ngCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAYYvtnpAFJCgoKqtf6W7du\ntTXPm2++ecf+Tp061WjbtWuXrbmmTJlS7zFz5szRtGnT6j0uMzOz3mMkyeFw2BqHxsURKgAYQqAC\ngCF3FahFRUWKjo5WVlaWJOns2bOaPHmyJk2apDlz5qiysrLGmDVr1mjChAmKiYnR4cOHzVYNAH6o\nzkC9evWqkpOTFRER4W3bsGGDJk2apB07dqhHjx7KycnxGVNYWKjTp08rOztbq1ev1urVq81XDgB+\nps5ADQwMVGZmptxut7etoKBATz/9tCRpxIgRys/P9xmTn5+v6OhoSVJYWJguXbqkiooKk3UDgN+p\nM1CdTqdat27t03bt2jUFBgZKkoKDg+XxeHz6y8rKfD51dblcNdYBgPtNgy+bupvf+ON3AO9fGzZs\naNT179b8+fON/V+lpaW2xm3ZssVYDWiebAVqUFCQrl+/rtatW6u0tNTn7QBJcrvdKisr8y6fO3dO\nISEhDasUfikhIaFe62dkZNia507Xoc6fP1/r1q2r0R4eHm5rLjvXoZaWltb7F2AlrkO939i6bGro\n0KHKy8uTJO3evVvDhg3z6Y+MjPT2Hz16VG63W23btm1gqQDg3+o8Qj1y5IhSU1NVUlIip9OpvLw8\npaena8mSJcrOzlbXrl01fvx4SdK8efOUkpKiAQMGKDw8XDExMXI4HEpKSmr0BwIA91qdgfrYY49p\n+/btNdq3bdtWo+2tt97y/nvhwoUNLA0Amhe+KQUAhjgsPoJHA9T3g5iuXbvamuf//u//bI2z4+DB\ng/Ue079/f4WGhtZ73LffflvvMVLtN4PBvccRKgAYQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAY\nQqACgCEEKgAYQqACgCEEKgAYQqACgCEEKgAY0uDflML/th9++KFe6zeHn8Lp2LFjk83ldPISvJ9w\nhAoAhhCoAGAIgQoAhhCoAGAIgQoAhhCoAGAIgQoAhhCoAGAIgQoAhtxVoBYVFSk6OlpZWVmSpLNn\nz2rq1KmKjY3V1KlT5fF4fNYvKCjQkCFDNHnyZE2ePFnJycnmKwcAP1Pn996uXr2q5ORkRUREeNve\nfvttvfzyyxo3bpw++ugjbdu2TYmJiT7jBg0apA0bNpivGAD8VJ1HqIGBgcrMzJTb7fa2JSUlacyY\nMZKkTp066eLFi41XIQA0E3UeoTqdzho3cAgKCpIkVVdXa8eOHZo1a1aNcSdOnND06dN16dIlxcfH\nKzIy0lDJ8Cc7d+681yUY99BDD9kaV1paargSNDe2b3VTXV2txMREDRkyxOftAEnq2bOn4uPjNXbs\nWBUXF2vKlCnavXu3AgMDG1ww/Etubm691v/9739va57//Oc/t+3r3bu3vv322xrtPXv2tDXXH/7w\nh3qPefPNNzV+/Ph6j/vrX/9a7zGS5HA4bI1D47L9Kf/SpUvVo0cPxcfH1+gLDQ3VuHHj5HA41L17\ndz344IP89QZw37MVqLm5uWrZsqUSEhJu279161ZJksfjUXl5uUJDQ+1XCQDNQJ2n/EeOHFFqaqpK\nSkrkdDqVl5en8vJytWrVSpMnT5YkhYWFadWqVZo3b55SUlI0cuRILVy4UP/85z9148YNrVq1itN9\nAPe9OgP1scce0/bt2+/qP3vrrbe8/964caP9qgCgGeKbUgBgCIEKAIYQqABgCIEKAIYQqABgCIEK\nAIYQqABgCIEKAIY4LMuy7nURaL7qu/usWbPG1jzLly+/Yw0mbxbyy1/+st5jTp48qe+++67e4+ze\nwAX+iSNUADCEQAUAQwhUADCEQAUAQwhUADCEQAUAQwhUADCEQAUAQwhUADCEQAUAQwhUADCEQAUA\nQwhUADCEu02hWbh8+fJt+9q3b19r//Xr123NFRISUu8xJu92heaLI1QAMIRABQBD7ipQi4qKFB0d\nraysLEnSkiVL9Oyzz2ry5MmaPHmy/v3vf9cYs2bNGk2YMEExMTE6fPiw0aIBwB8561rh6tWrSk5O\nVkREhE/7/PnzNWLEiFrHFBYW6vTp08rOztbJkye1bNkyZWdnm6kYAPxUnUeogYGByszMlNvtvuv/\nND8/X9HR0ZKksLAwXbp0SRUVFfarBIBmoM4jVKfTKaez5mpZWVnatm2bgoODtWLFCrlcLm9fWVmZ\nwsPDvcsul0sej0dt27Y1VDb+17Rv377e/XWNAUyrM1Br8/zzz6tjx47q06ePNm/erHfeeUcrV668\n7fpcmYWG4rIpNAe2PuWPiIhQnz59JEkjR45UUVGRT7/b7VZZWZl3+dy5c7Z2UgBoTmwF6uzZs1Vc\nXCxJKigoUK9evXz6IyMjlZeXJ0k6evSo3G43p/sA7nt1nvIfOXJEqampKikpkdPpVF5enmJjYzV3\n7lw98MADCgoKUkpKiiRp3rx5SklJ0YABAxQeHq6YmBg5HA4lJSU1+gMBgHuNr56iWeA9VDQHfFMK\nAAzhCBUADOEIFQAMIVABwBACFQAMIVABwBACFQAMIVABwBACFQAMIVABwBACFQAMIVABwBACFQAM\nIVABwBACFQAMIVABwBACFQAMIVABwBACFQAMIVABwBACFQAMIVABwBACFQAMIVABwBACFQAMcd7N\nSkVFRZo5c6amTp2q2NhYJSQk6MKFC5Kkixcv6oknnlBycrJ3/Y8//ljr169X9+7dJUlDhw7VjBkz\nGqF8APAfdQbq1atXlZycrIiICG/bhg0bvP9eunSpXnrppRrjxo0bp8WLFxsqEwD8X52n/IGBgcrM\nzJTb7a7Rd+rUKV25ckX9+vVrlOIAoDmpM1CdTqdat25da9+HH36o2NjYWvsKCwsVFxenV155RceO\nHWtYlQDQDNzVe6i1qays1IEDB7Rq1aoafY8//rhcLpeeeuopHTx4UIsXL9Ynn3zSkDoBwO/ZDtT9\n+/ff9lQ/LCxMYWFhkqT+/fvr/Pnzqq6uVkBAgN3pAMDv2b5s6ptvvlHv3r1r7cvMzNSnn34q6eYV\nAi6XizAFcN+r8wj1yJEjSk1NVUlJiZxOp/Ly8pSRkSGPx+O9LOonM2bM0Hvvvadnn31WixYt0p//\n/GdVVVVp9erVjfYAAMBfOCzLsu51EQBwP+CbUgBgCIEKAIYQqABgCIEKAIYQqABgCIEKAIYQqABg\nCIEKAIYQqABgCIEKAIYQqABgCIEKAIYQqABgCIEKAIYQqABgCIEKAIYQqABgCIEKAIYQqABgCIEK\nAIYQqABgCIEKAIYQqABgCIEKAIYQqABgiPNuVkpLS9OBAwdUVVWladOmqW/fvkpMTFR1dbVCQkK0\ndu1aBQYG+oxZs2aNDh06JIfDoWXLlqlfv36N8gAAwF/UGaj79u3T8ePHlZ2drQsXLuiFF15QRESE\nJk2apLFjx2rdunXKycnRpEmTvGMKCwt1+vRpZWdn6+TJk1q2bJmys7Mb9YEAwL1W5yn/wIEDtX79\neklS+/btde3aNRUUFOjpp5+WJI0YMUL5+fk+Y/Lz8xUdHS1JCgsL06VLl1RRUWG6dgDwK3UGakBA\ngIKCgiRJOTk5ioqK0rVr17yn+MHBwfJ4PD5jysrK1KlTJ++yy+WqsQ4A3G/u+kOpzz//XDk5OVq5\ncqVPu2VZdY69m3UAoLm7q0D98ssvtXHjRmVmZqpdu3YKCgrS9evXJUmlpaVyu90+67vdbpWVlXmX\nz507p5CQEINlA4D/qTNQr1y5orS0NG3atEkdO3aUJA0dOlR5eXmSpN27d2vYsGE+YyIjI739R48e\nldvtVtu2bU3XDgB+pc5P+T/77DNduHBBc+fO9ba98cYbWr58ubKzs9W1a1eNHz9ekjRv3jylpKRo\nwIABCg8PV0xMjBwOh5KSkhrvEQCAn3BYvMEJAEbwTSkAMIRABQBDCFQAMIRABQBDCFQAMIRABQBD\nCFQAMIRABQBDCFQAMIRABQBDCFQAMIRABQBDCFQAMIRABQBDCFQAMIRABQBDCFQAMIRABQBDCFQA\nMIRABQBDCFQAMIRABQBDnPe6gFutWbNGhw4dksPh0LJly9SvXz9v3969e7Vu3ToFBAQoKipKs2bN\narQ60tLSdODAAVVVVWnatGkaPXq0t2/kyJHq3LmzAgICJEnp6ekKDQ01XkNBQYHmzJmjXr16SZIe\nfvhhrVixwtvfVNtj165dys3N9S4fOXJEBw8e9C6Hh4drwIAB3uUPPvjAu21MKCoq0syZMzV16lTF\nxsbq7NmzSkxMVHV1tUJCQrR27VoFBgb6jLnTfmSyjqVLl6qqqkpOp1Nr165VSEiId/26nj9TdSxZ\nskRHjx5Vx44dJUlxcXF66qmnfMY0xfZISEjQhQsXJEkXL17UE088oeTkZO/6H3/8sdavX6/u3btL\nkoYOHaoZM2Y0uI6fv1b79u17T/YPL8tPFBQUWK+99pplWZZ14sQJ6+WXX/bpHzt2rPXf//7Xqq6u\ntiZOnGgdP368UerIz8+3Xn31VcuyLOv8+fPW8OHDffpHjBhhVVRUNMrct9q3b581e/bs2/Y31fa4\nVUFBgbVq1SqftkGDBjXafD/88IMVGxtrLV++3Nq+fbtlWZa1ZMkS67PPPrMsy7LefPNN66OPPqpR\n4532I1N1JCYmWn//+98ty7KsrKwsKzU11WdMXc+fqToWL15s/etf/7rtmKbaHrdasmSJdejQIZ+2\nv/zlL9Ybb7zR4LlvVdtr9V7sH7fym1P+/Px8RUdHS5LCwsJ06dIlVVRUSJKKi4vVoUMHdenSRS1a\ntNDw4cOVn5/fKHUMHDhQ69evlyS1b99e165dU3V1daPMZVdTbo9bvfvuu5o5c2ajz/OTwMBAZWZm\nyu12e9sKCgr09NNPS5JGjBhR43HfaT8yWUdSUpLGjBkjSerUqZMuXrzYoDns1lGXptoePzl16pSu\nXLli9qjvNmp7rd6L/eNWfhOoZWVl6tSpk3fZ5XLJ4/FIkjwej1wuV619pgUEBCgoKEiSlJOTo6io\nqBqnsElJSZo4caLS09NlWVaj1CFJJ06c0PTp0zVx4kTt2bPH296U2+Mnhw8fVpcuXXxOayWpsrJS\nCxYsUExMjLZt22Z0TqfTqdatW/u0Xbt2zXsKFxwcXONx32k/MllHUFCQAgICVF1drR07dujZZ5+t\nMe52z5/JOiQpKytLU6ZM0bx583T+/HmfvqbaHj/58MMPFRsbW2tfYWGh4uLi9Morr+jYsWMNqkGq\n/bV6L/aPW/nVe6i3asyguhuff/65cnJy9P777/u0JyQkaNiwYerQoYNmzZqlvLw8PfPMM8bn79mz\np+Lj4zV27FgVFxdrypQp2r17d433g5pKTk6OXnjhhRrtiYmJeu655+RwOBQbG6tf//rX6tu3b5PU\ndDf7SGPuR9XV1UpMTNSQIUMUERHh09dUz9/zzz+vjh07qk+fPtq8ebPeeecdrVy58rbrN+b2qKys\n1IEDB7Rq1aoafY8//rhcLpeeeuopHTx4UIsXL9Ynn3xiZN5bX6u3ft5xL/YPvzlCdbvdKisr8y6f\nO3fOezT0877S0tJ6nfbU15dffqmNGzcqMzNT7dq18+kbP368goOD5XQ6FRUVpaKiokapITQ0VOPG\njZPD4VD37t314IMPqrS0VFLTbw/p5ql2//79a7RPnDhRbdq0UVBQkIYMGdJo2+MnQUFBun79uqTa\nH/ed9iPTli5dqh49eig+Pr5G352eP5MiIiLUp08fSTc/MP359m/K7bF///7bnuqHhYV5Pyzr37+/\nzp8/b+SttJ+/Vu/1/uE3gRoZGam8vDxJ0tGjR+V2u9W2bVtJUrdu3VRRUaEzZ86oqqpKX3zxhSIj\nIxuljitXrigtLU2bNm3yfnJ6a19cXJwqKysl3dyBfvoU17Tc3Fxt3bpV0s1T/PLycu/VBE25PaSb\nO2abNm1qHF2dOnVKCxYskGVZqqqq0tdff91o2+MnQ4cO9e4nu3fv1rBhw3z677QfmZSbm6uWLVsq\nISHhtv23e/5Mmj17toqLiyXd/KP38+3fVNtDkr755hv17t271r7MzEx9+umnkm5eIeByuRp8NUht\nr9V7vX84rHt9bn2L9PR0ffXVV3I4HEpKStKxY8fUrl07jRo1Svv371d6erokafTo0YqLi2uUGrKz\ns5WRkaGHHnrI2zZ48GA98sgjGjVqlP70pz/pb3/7m1q1aqVHH31UK1askMPhMF5HRUWFFi5cqMuX\nL+vGjRuKj49XeXl5k28P6ealUm+//ba2bNkiSdq8ebMGDhyo/v37a+3atdq3b59atGihkSNHGrkU\n5tZ5U1NTVVJSIqfTqdDQUKWnp2vJkiX68ccf1bVrV6WkpKhly5aaN2+eUlJS1Lp16xr70e1e5A2p\no7y8XK1atfK+GMPCwrRq1SpvHVVVVTWev+HDhxuvIzY2Vps3b9YDDzygoKAgpaSkKDg4uMm3R0ZG\nhjIyMvTkk09q3Lhx3nVnzJih9957T99//70WLVrk/eNr4nKl2l6rb7zxhpYvX96k+8et/CpQAaA5\n85tTfgBo7ghUADCEQAUAQwhUADCEQAUAQwhUADCEQAUAQwhUADDk/wEg+DWmBPS47QAAAABJRU5E\nrkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f684c35b2d0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"PaSuGk_zM0DM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# get opencv\n","!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n","import cv2\n","\n","from scipy import ndimage"],"execution_count":0,"outputs":[]},{"metadata":{"id":"14vH-1-jlGk1","colab_type":"text"},"cell_type":"markdown","source":["TODO: FIX\n","https://medium.com/@o.kroeger/tensorflow-mnist-and-your-own-handwritten-digits-4d1cd32bbab4"]},{"metadata":{"id":"IC_EBpv9UeL7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{},{}],"base_uri":"https://localhost:8080/","height":269},"outputId":"857bd333-7b16-44da-c22e-96c1c4500b28","executionInfo":{"status":"error","timestamp":1520273109860,"user_tz":300,"elapsed":453,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["# image proc\n","nparr = np.fromstring(img_str, np.uint8)\n","\n","img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n","\n","gray = cv2.resize(255-img, (28,28))\n","(thresh, gray) = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n","\n","while np.sum(gray[0]) == 0:\n","    gray = gray[1:]\n","\n","while np.sum(gray[:,0]) == 0:\n","    gray = np.delete(gray,0,1)\n","\n","while np.sum(gray[-1]) == 0:\n","    gray = gray[:-1]\n","\n","while np.sum(gray[:,-1]) == 0:\n","    gray = np.delete(gray,-1,1)\n","\n","rows,cols = gray.shape\n","\n","if rows > cols:\n","    factor = 20.0/rows\n","    rows = 20\n","    cols = int(round(cols*factor))\n","    gray = cv2.resize(gray, (cols,rows))\n","else:\n","    factor = 20.0/cols\n","    cols = 20\n","    rows = int(round(rows*factor))\n","    gray = cv2.resize(gray, (cols, rows))\n","\n","colsPadding = (int(math.ceil((28-cols)/2.0)),int(math.floor((28-cols)/2.0)))\n","rowsPadding = (int(math.ceil((28-rows)/2.0)),int(math.floor((28-rows)/2.0)))\n","gray = np.lib.pad(gray,(rowsPadding,colsPadding),'constant')\n","\n","def getBestShift(img):\n","    cy,cx = ndimage.measurements.center_of_mass(img)\n","\n","    rows,cols = img.shape\n","    shiftx = np.round(cols/2.0-cx).astype(int)\n","    shifty = np.round(rows/2.0-cy).astype(int)\n","\n","    return shiftx,shifty\n","def shift(img,sx,sy):\n","    rows,cols = img.shape\n","    M = np.float32([[1,0,sx],[0,1,sy]])\n","    shifted = cv2.warpAffine(img,M,(cols,rows))\n","    return shifted\n","gray = np.lib.pad(gray,(rowsPadding,colsPadding),'constant')\n","shiftx,shifty = getBestShift(gray)\n","shifted = shift(gray,shiftx,shifty)\n","gray = shifted\n","\n","\n","flatten = gray.flatten() / 255.0\n","img_asarr = np.zeros((1,784))\n","\n","img_asarr[0] = flatten\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-60-34fc634a4a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mimg_asarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mimg_asarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (1476) into shape (784)"]}]},{"metadata":{"id":"G0KaZ0AvLWz_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{},{}],"base_uri":"https://localhost:8080/","height":105},"outputId":"b2f530ad-4ff6-4b3b-fdf2-631936bec59a","executionInfo":{"status":"ok","timestamp":1520272703004,"user_tz":300,"elapsed":581,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["# gray = cv2.imread(dd, cv2.IMREAD_GRAYSCALE)\n","\n","# file_bytes = np.asarray(bytearray(dd), dtype=np.uint8)\n","# nparr = np.fromstring(img_str, np.uint8)\n","\n","# img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n","\n","# gray = cv2.resize(255-img, (28,28))\n","# flatten = gray.flatten() / 255.0\n","# img_asarr = np.zeros((1,784))\n","# img_asarr[0] = flatten\n","\n","# corvals = np.zeros((1,10))\n","# corval = np.zeros((10))\n","# corval[6] = 1\n","# corvals[0] = corval\n","\n","# print x, y_\n","\n","# pred = tf.argmax(y,1)\n","\n","tf.initialize_all_variables().run()\n","with tf.Session() as sess:\n","  tf.initialize_all_variables().run()\n","  label = sess.run(y, \n","                   feed_dict={x: img_asarr})\n","  print label\n","#   print sess.run(pred, feed_dict={x: img_asarr, y_: corvals})\n","#   print sess.run(accuracy, feed_dict={x: img_asarr, y_: corvals})\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"stream","text":["Tensor(\"Placeholder:0\", shape=(?, 784), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(?, 10), dtype=float32)\n","[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"metadata":{"id":"xPKsX5qHuBO1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{},{}],"base_uri":"https://localhost:8080/","height":728},"outputId":"96e06a05-2d0d-4a86-c8c3-c563ee1e4ca0","executionInfo":{"status":"error","timestamp":1520270066165,"user_tz":300,"elapsed":429,"user":{"displayName":"Kaustav Haldar","photoUrl":"//lh6.googleusercontent.com/-SGJ1BLtauTk/AAAAAAAAAAI/AAAAAAAAAAA/4Cxjig_XDts/s50-c-k-no/photo.jpg","userId":"105154027733464709839"}}},"cell_type":"code","source":["print len(mnist.test.images[0])\n","# x = tf.Variable(pil_im, name='x')\n","# print x\n","# x = tf.reshape(x, shape=[-1])\n","# print x\n","# im = double(rgb2gray(pil_im))\n","# im = im(:)\n","# im = im./max(im)\n","# x = im\n","# x = pil_im\n","# x = tf.image.encode_png(x)\n","# x = np.array(x)\n","# x = x.ravel()\n","# x = tf.placeholder(x)\n","from array import *\n","# data_image = array('B')\n","# pixel = pil_im.load()\n","# width, height = pil_im.size\n","\n","# for x in range(0,width):\n","#   for y in range(0,height):\n","#     data_image.append(pixel[y,x])\n","\n","# x = data_image\n","\n","\n","# imagedata = np.zeros((-1,28,28), dtype=np.uint8)\n","# import matplotlib.image as imagick\n","# x = imagick.inread(pil_im)\n","# x = x.shape\n","\n","# x = np.array(x, dtype=np.uint8)\n","# x = x.shape\n","# x = x.ravel()\n","# x = hash(tuple(x))\n","# x = np.matrix(x)\n","\n","with tf.Session() as sess:\n","  new_image_label= sess.run(y, \n","                            feed_dict={x: dd})\n","print new_image_label"],"execution_count":0,"outputs":[{"output_type":"stream","text":["784\n"],"name":"stdout"},{"output_type":"error","ename":"InternalError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mInternalError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-41-0e25ecb78ccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   new_image_label= sess.run(y, \n\u001b[0;32m---> 40\u001b[0;31m                             feed_dict={x: dd})\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnew_image_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInternalError\u001b[0m: Unable to get element as bytes."]}]},{"metadata":{"id":"ZdOrJXkoOPh-","colab_type":"text"},"cell_type":"markdown","source":["# Glossary and links\n","\n","Difference between trainers well explained  \n","https://stackoverflow.com/questions/36162180/gradient-descent-vs-adagrad-vs-momentum-in-tensorflow\n","\n","Sourcefor nielsennet\n","https://medium.com/initialized-capital/we-need-to-go-deeper-a-practical-guide-to-tensorflow-and-inception-50e66281804f\n","https://github.com/initialized/tensorflow-tutorial/blob/master/mnist-slim/MNIST%20Slim.ipynb\n","\n","\n","TODO: figure out mcdnn\n","https://arxiv.org/pdf/1202.2745.pdf\n","https://stackoverflow.com/questions/41990014/load-multiple-models-in-tensorflow"]}]}